{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d208146b-2b12-4c14-89eb-fe87cf9b5825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 03:02:54.349251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 03:02:54.515638: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-12 03:02:55.264615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-12 03:02:55.264722: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-12 03:02:55.264732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "2022-12-12 03:02:59.029703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.041001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.042742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.045414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 03:02:59.045980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.047689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.049348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.716908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.718717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.720251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 03:02:59.721685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-12-12 03:03:00.428954: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
      "2022-12-12 03:03:02.008518: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n",
      "2022-12-12 03:03:03.779379: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 4\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss:3.653031\n",
      "Time Taken:  4.084663391113281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 21). These functions will not be directly callable after loading.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:525: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  \"imported and registered.\" % type_spec_class_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/savedPolicy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/savedPolicy/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 21). These functions will not be directly callable after loading.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:525: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  \"imported and registered.\" % type_spec_class_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/eval_policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/eval_policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saced in: \n",
      " /home/jupyter/savedModel/cpkt-1\n",
      "1000/1000\n",
      "GIF directory:  /home/jupyterimages/rl/myAgentPlays-2.gif/myAgentPlays-2.gif\n",
      "GIF name is changed so as not to coincide with original GIF submission.\n",
      "\n",
      "------------Program Execution Complete-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "# %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "import gym\n",
    "# gym.envs.registry.all()\n",
    "\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "game_name = \"SpaceInvaders-v0\"\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "   \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "import ale_py\n",
    "\n",
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "import logging\n",
    "\n",
    "env = gym.make(game_name)\n",
    "# print(env)\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "# env.step(1) # Fire\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env.unwrapped\n",
    "\n",
    "       \n",
    "limited_repeating_env = suite_gym.load(\n",
    "    game_name,\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=100)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"SpaceInvaders-v0\"\n",
    "\n",
    "class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n",
    "    def skip_frames(self, num_skip):\n",
    "        for _ in range(num_skip):\n",
    "          super().step(0) # NOOP for num_skip steps\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.skip_frames(40)\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        lives_before_action = self.ale.lives()\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "        if self.ale.lives() < lives_before_action and not done:\n",
    "            self.skip_frames(40)\n",
    "        return obs, rewards, done, info\n",
    "    \n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
    "            \n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def create_gif(frames):\n",
    "    gif_name = \"myAgentPlays-2.gif\" \n",
    "    image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "    # frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "    frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "    frame_images[0].save(image_path, format='GIF',\n",
    "                         append_images=frame_images[1:],\n",
    "                         save_all=True,\n",
    "                         duration=30,\n",
    "                         loop=0)\n",
    "    print(\"\\nGIF directory: \", os.getcwd() + image_path + '/' + gif_name)\n",
    "    print(\"GIF name is changed so as not to coincide with original GIF submission.\")\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "            # train_checkpointer.save(agent.train_step_counter)\n",
    "        # print(\"iteration: \", iteration)\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessingWithSkipStart, FrameStack4])\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "# for _ in range(4):\n",
    "#     time_step = env.step(3) # LEFT\n",
    "\n",
    "# Building the Agent:\n",
    "tf_env = TFPyEnvironment(env)\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n",
    "train_step = tf.Variable(0) # globak_step substitute\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=25000// update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()\n",
    "\n",
    "# Building the Replay Buffer:\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'lastModelCheckpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch\n",
    "\n",
    "# Creating Train Metrics:\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "\n",
    "# Log Metrics:\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration\n",
    "\n",
    "\n",
    "trajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n",
    "    sample_batch_size=2,\n",
    "    num_steps=3,\n",
    "    single_deterministic_pass=False)))\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "start_time = time.time()\n",
    "train_agent(n_iterations=10)\n",
    "print(\"\\nTime Taken: \", time.time() - start_time)\n",
    "# beep_when_finished()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "eval_policy_dir = os.path.join(os.getcwd(), 'eval_policy')\n",
    "policy_dir = os.path.join(os.getcwd(), 'savedPolicy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(eval_policy)\n",
    "tf_policy_saver.save(policy_dir)\n",
    "tf_policy_saver.save(eval_policy_dir)\n",
    "train_checkpointer.save(train_step)\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"savedModel\", \"cpkt\")\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "model_checkpoint = tf.train.Checkpoint(model = agent, step = train_step)\n",
    "saved_path = model_checkpoint.save(file_prefix = checkpoint_path)\n",
    "print(\"Model saced in: \\n\", saved_path)\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "num_frames = 1000\n",
    "    \n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(num_frames)],\n",
    "    num_steps=num_frames)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "# anim = plot_animation(frames)\n",
    "\n",
    "create_gif(frames)\n",
    "\n",
    "print(\"\\n------------Program Execution Complete-------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0034e35-370f-4e68-8710-a9e2fdbba688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
