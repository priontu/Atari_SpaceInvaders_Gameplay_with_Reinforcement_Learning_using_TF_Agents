{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9811ec0-8a7b-4d67-a160-d15b9f52b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<AtariEnv<SpaceInvaders-v0>>>\n",
      "ActionClipWrapper           Wraps an environment and clips actions to spec before applying.\n",
      "ActionDiscretizeWrapper     Wraps an environment with continuous actions and discretizes them.\n",
      "ActionOffsetWrapper         Offsets actions to be zero-based.\n",
      "ActionRepeat                Repeates actions over n-steps while acummulating the received reward.\n",
      "ExtraDisabledActionsWrapper Adds extra unavailable actions.\n",
      "FixedLength                 Truncates long episodes and pads short episodes to have a fixed length.\n",
      "FlattenActionWrapper        Flattens the action.\n",
      "FlattenObservationsWrapper  Wraps an environment and flattens nested multi-dimensional observations.\n",
      "GoalReplayEnvWrapper        Adds a goal to the observation, used for HER (Hindsight Experience Replay).\n",
      "HistoryWrapper              Adds observation and action history to the environment's observations.\n",
      "ObservationFilterWrapper    Filters observations based on an array of indexes.\n",
      "OneHotActionWrapper         Converts discrete action to one_hot format.\n",
      "PerformanceProfiler         End episodes after specified number of steps.\n",
      "PyEnvironmentBaseWrapper    PyEnvironment wrapper forwards calls to the given environment.\n",
      "RunStats                    Wrapper that accumulates run statistics as the environment iterates.\n",
      "TimeLimit                   End episodes after specified number of steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 23:53:07.517350: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n",
      "INFO:absl:Checkpoint available: /home/jupyter/lastModelCheckpoint/ckpt-2824320\n",
      "2022-12-11 23:53:08.265715: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17800/20000"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import PIL\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import jupyter_beeper\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "import tf_agents.environments.wrappers\n",
    "\n",
    "import gym\n",
    "# gym.envs.registry.all()\n",
    "\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "game_name = \"SpaceInvaders-v0\"\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "    \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "import ale_py\n",
    "\n",
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "import tf_agents.environments.wrappers\n",
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)\n",
    "\n",
    "env = gym.make(game_name)\n",
    "print(env)\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "# env.step(1) # Fire\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env.unwrapped\n",
    "\n",
    "for name in dir(tf_agents.environments.wrappers):\n",
    "    obj = getattr(tf_agents.environments.wrappers, name)\n",
    "    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))\n",
    "\n",
    "        \n",
    "limited_repeating_env = suite_gym.load(\n",
    "    game_name,\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=100)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"SpaceInvaders-v0\"\n",
    "\n",
    "class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n",
    "    def skip_frames(self, num_skip):\n",
    "        for _ in range(num_skip):\n",
    "          super().step(0) # NOOP for num_skip steps\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.skip_frames(40)\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        lives_before_action = self.ale.lives()\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "        if self.ale.lives() < lives_before_action and not done:\n",
    "            self.skip_frames(40)\n",
    "        return obs, rewards, done, info\n",
    "\n",
    "def plot_observation(obs):\n",
    "    # Since there are only 3 color channels, you cannot display 4 frames\n",
    "    # with one primary color per frame. So this code computes the delta between\n",
    "    # the current frame and the mean of the other frames, and it adds this delta\n",
    "    # to the red and blue channels to get a pink color for the current frame.\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")    \n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessingWithSkipStart, FrameStack4])\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "# for _ in range(4):\n",
    "#     time_step = env.step(3) # LEFT\n",
    "    \n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "\n",
    "limited_repeating_env = suite_gym.load(\n",
    "    game_name,\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=100)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"SpaceInvaders-v0\"\n",
    "# SpaceInvadersNoFrameskip-v4\n",
    "\n",
    "class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n",
    "    def skip_frames(self, num_skip):\n",
    "        for _ in range(num_skip):\n",
    "          super().step(0) # NOOP for num_skip steps\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.skip_frames(40)\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        lives_before_action = self.ale.lives()\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "        if self.ale.lives() < lives_before_action and not done:\n",
    "            self.skip_frames(40)\n",
    "        return obs, rewards, done, info\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")    \n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def create_gif(frames):\n",
    "    gif_name = \"myAgentPlays.gif\" \n",
    "    image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "    # frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "    frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "    frame_images[0].save(image_path, format='GIF',\n",
    "                         append_images=frame_images[1:],\n",
    "                         save_all=True,\n",
    "                         duration=30,\n",
    "                         loop=0)\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessingWithSkipStart, FrameStack4])\n",
    "\n",
    "\n",
    "tf_env = TFPyEnvironment(env)\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "train_step = tf.Variable(0) # globak_step substitute\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=25000// update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'lastModelCheckpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration\n",
    "\n",
    "ale_frames = 20000\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(ale_frames)],\n",
    "    num_steps=ale_frames) # <=> 80,000 ALE frames\n",
    "\n",
    "final_time_step, final_policy_state = init_driver.run()\n",
    "\n",
    "tf.random.set_seed(9) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "#trajectories, buffer_info = replay_buffer.get_next( # get_next() is deprecated\n",
    "#    sample_batch_size=2, num_steps=3)\n",
    "\n",
    "trajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n",
    "    sample_batch_size=2,\n",
    "    num_steps=3,\n",
    "    single_deterministic_pass=False)))\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "start_time = time.time()\n",
    "train_agent(n_iterations=10)\n",
    "print(\"\\nTime Taken: \", time.time() - start_time)\n",
    "# beep_when_finished()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "# eval_policy_dir = os.path.join(os.getcwd(), 'eval_policy')\n",
    "# policy_dir = os.path.join(os.getcwd(), 'savedPolicy')\n",
    "# tf_policy_saver = policy_saver.PolicySaver(eval_policy)\n",
    "# tf_policy_saver.save(policy_dir)\n",
    "# tf_policy_saver.save(eval_policy_dir)\n",
    "# train_checkpointer.save(train_step)\n",
    "# checkpoint_path = os.path.join(os.getcwd(), \"savedModel\", \"cpkt\")\n",
    "# os.makedirs(checkpoint_path, exist_ok=True)\n",
    "# model_checkpoint = tf.train.Checkpoint(model = agent, step = train_step)\n",
    "# saved_path = model_checkpoint.save(file_prefix = checkpoint_path)\n",
    "# print(\"Model saced in: \\n\", saved_path)\n",
    "\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "num_frames = 1000\n",
    "    \n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(num_frames)],\n",
    "    num_steps=num_frames)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "create_gif(frames)\n",
    "\n",
    "anim = plot_animation(frames)\n",
    "\n",
    "print(\"\\n------------Program Execution Complete-------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54df8e-c608-4da3-b185-dce38595fe26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
